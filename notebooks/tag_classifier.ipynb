{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import load_json\n",
    "from data_loader.song_info_loader import SongMetaInfoLoader\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../res/orig/train.json'\n",
    "META_DATA_PATH = '../res/song_meta.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make meta info of songs!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b93d0f71da48d19341ebe59f0e7fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=92056.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c77ea53b6544ca2975f9f5ac748dbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=549729.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c524bfefdad2439498fbdc7521cc8bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=707989.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train_data = load_json(TRAIN_DATA_PATH)\n",
    "meta_data = SongMetaInfoLoader()\n",
    "meta_data.make_info_map(playlist_data=train_data, meta_data=load_json(META_DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_album_id_dict = dict(enumerate(sorted(set(meta_data.song_album_id_map.values()))))\n",
    "album_id_aid_dict = dict(map(lambda kv: (kv[1], kv[1]), aid_album_id_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playlist_to_aids(playlist):\n",
    "    aid_list = []\n",
    "    for plays in tqdm(playlist):\n",
    "        aids = list(map(lambda s: str(meta_data.song_album_id_map[s]), plays['songs']))\n",
    "        aid_list.append(aids)\n",
    "        \n",
    "    return aid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbba9402a824484b532c9a0641b1df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=92056.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aid_seqs = playlist_to_aids(playlist=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4239978"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda e: len(e), aid_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_counter = Counter([aid for sub_seq in aid_seqs for aid in sub_seq]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 2161126, 4071, 0.096\n",
      "1000, 311295, 544, 21.738\n",
      "2000, 10263118, 352, 31.953\n",
      "3000, 10252753, 255, 38.985\n",
      "4000, 2125020, 199, 44.305\n",
      "5000, 10094403, 162, 48.535\n",
      "6000, 10292688, 135, 52.012\n",
      "7000, 2179131, 116, 54.959\n",
      "8000, 10377359, 102, 57.534\n",
      "9000, 2681085, 90, 59.796\n",
      "10000, 2138528, 81, 61.806\n",
      "11000, 10037829, 73, 63.615\n",
      "12000, 2317098, 66, 65.243\n",
      "13000, 2283226, 60, 66.722\n",
      "14000, 10034886, 55, 68.078\n",
      "15000, 2704355, 51, 69.325\n",
      "16000, 10184702, 47, 70.478\n",
      "17000, 2039758, 44, 71.550\n",
      "18000, 348865, 41, 72.546\n",
      "19000, 10137358, 38, 73.482\n",
      "20000, 307320, 36, 74.358\n",
      "21000, 2333097, 34, 75.181\n",
      "22000, 2687645, 32, 75.956\n",
      "23000, 15883, 30, 76.690\n",
      "24000, 2304486, 29, 77.384\n",
      "25000, 10138006, 27, 78.038\n",
      "26000, 2168047, 26, 78.661\n",
      "27000, 2304302, 25, 79.255\n",
      "28000, 2193730, 23, 79.820\n",
      "29000, 317519, 22, 80.360\n",
      "30000, 397298, 21, 80.877\n",
      "31000, 17090, 20, 81.372\n",
      "32000, 2639445, 20, 81.844\n",
      "33000, 1128955, 19, 82.296\n",
      "34000, 10196411, 18, 82.731\n",
      "35000, 10098925, 17, 83.152\n",
      "36000, 2019246, 17, 83.553\n",
      "37000, 2261670, 16, 83.940\n",
      "38000, 10152530, 16, 84.318\n",
      "39000, 2336026, 15, 84.673\n",
      "40000, 512151, 15, 85.027\n",
      "41000, 10028279, 14, 85.358\n",
      "42000, 2268385, 14, 85.688\n",
      "43000, 10180171, 13, 85.997\n",
      "44000, 148282, 13, 86.303\n",
      "45000, 10105287, 12, 86.598\n",
      "46000, 10021187, 12, 86.881\n",
      "47000, 10009062, 12, 87.164\n",
      "48000, 10031677, 11, 87.429\n",
      "49000, 2262921, 11, 87.688\n",
      "50000, 10054894, 11, 87.948\n",
      "51000, 333174, 10, 88.191\n",
      "52000, 35378, 10, 88.427\n",
      "53000, 312597, 10, 88.663\n",
      "54000, 2328327, 9, 88.897\n",
      "55000, 165224, 9, 89.109\n",
      "56000, 316155, 9, 89.321\n",
      "57000, 352334, 9, 89.534\n",
      "58000, 2325207, 9, 89.746\n",
      "59000, 127778, 8, 89.935\n",
      "60000, 2328588, 8, 90.124\n",
      "61000, 2682763, 8, 90.312\n",
      "62000, 10220271, 8, 90.501\n",
      "63000, 305311, 7, 90.685\n",
      "64000, 2651605, 7, 90.850\n",
      "65000, 10077224, 7, 91.015\n",
      "66000, 343499, 7, 91.180\n",
      "67000, 2684921, 7, 91.346\n",
      "68000, 2011227, 7, 91.511\n",
      "69000, 10039465, 6, 91.665\n",
      "70000, 4805, 6, 91.806\n",
      "71000, 44657, 6, 91.948\n",
      "72000, 10136965, 6, 92.089\n",
      "73000, 927958, 6, 92.231\n",
      "74000, 2185339, 6, 92.373\n",
      "75000, 2296096, 6, 92.514\n",
      "76000, 10243860, 5, 92.652\n",
      "77000, 10270653, 5, 92.770\n",
      "78000, 10301279, 5, 92.888\n",
      "79000, 46108, 5, 93.006\n",
      "80000, 2657488, 5, 93.124\n",
      "81000, 20434, 5, 93.242\n",
      "82000, 386718, 5, 93.360\n",
      "83000, 10068623, 5, 93.478\n",
      "84000, 10029026, 5, 93.596\n",
      "85000, 589457, 5, 93.714\n",
      "86000, 2974768, 4, 93.817\n",
      "87000, 307722, 4, 93.912\n",
      "88000, 2076448, 4, 94.006\n",
      "89000, 2134606, 4, 94.100\n",
      "90000, 2647078, 4, 94.195\n",
      "91000, 2687653, 4, 94.289\n",
      "92000, 2253740, 4, 94.383\n",
      "93000, 353936, 4, 94.478\n",
      "94000, 10010943, 4, 94.572\n",
      "95000, 2304888, 4, 94.666\n",
      "96000, 10253004, 4, 94.761\n",
      "97000, 378496, 4, 94.855\n",
      "98000, 10043375, 4, 94.949\n",
      "99000, 10000228, 3, 95.034\n",
      "100000, 10362390, 3, 95.105\n",
      "101000, 2179638, 3, 95.176\n",
      "102000, 10301576, 3, 95.246\n",
      "103000, 303021, 3, 95.317\n",
      "104000, 10210002, 3, 95.388\n",
      "105000, 10332514, 3, 95.459\n",
      "106000, 10024878, 3, 95.529\n",
      "107000, 10188927, 3, 95.600\n",
      "108000, 2293425, 3, 95.671\n",
      "109000, 2127668, 3, 95.742\n",
      "110000, 10413570, 3, 95.812\n",
      "111000, 10281019, 3, 95.883\n",
      "112000, 2320022, 3, 95.954\n",
      "113000, 2336482, 3, 96.025\n",
      "114000, 10247820, 3, 96.095\n",
      "115000, 192854, 3, 96.166\n",
      "116000, 10016328, 3, 96.237\n",
      "117000, 2286300, 3, 96.308\n",
      "118000, 2171971, 3, 96.379\n",
      "119000, 390157, 2, 96.432\n",
      "120000, 10211449, 2, 96.479\n",
      "121000, 4309936, 2, 96.526\n",
      "122000, 10019417, 2, 96.573\n",
      "123000, 2646128, 2, 96.621\n",
      "124000, 2010967, 2, 96.668\n",
      "125000, 2320238, 2, 96.715\n",
      "126000, 10410241, 2, 96.762\n",
      "127000, 3801533, 2, 96.809\n",
      "128000, 2670326, 2, 96.856\n",
      "129000, 10258817, 2, 96.904\n",
      "130000, 10320115, 2, 96.951\n",
      "131000, 2696978, 2, 96.998\n",
      "132000, 2172793, 2, 97.045\n",
      "133000, 10334776, 2, 97.092\n",
      "134000, 3948772, 2, 97.139\n",
      "135000, 352962, 2, 97.187\n",
      "136000, 2646067, 2, 97.234\n",
      "137000, 10261167, 2, 97.281\n",
      "138000, 2151969, 2, 97.328\n",
      "139000, 10374547, 2, 97.375\n",
      "140000, 2278278, 2, 97.422\n",
      "141000, 28875, 2, 97.470\n",
      "142000, 1230753, 2, 97.517\n",
      "143000, 2645177, 2, 97.564\n",
      "144000, 10392621, 2, 97.611\n",
      "145000, 2291333, 2, 97.658\n",
      "146000, 3288839, 2, 97.705\n",
      "147000, 2665396, 2, 97.753\n",
      "148000, 4166574, 2, 97.800\n",
      "149000, 10025481, 2, 97.847\n",
      "150000, 2175387, 2, 97.894\n",
      "151000, 10166687, 2, 97.941\n",
      "152000, 10245888, 2, 97.989\n",
      "153000, 5125584, 1, 98.024\n",
      "154000, 409758, 1, 98.047\n",
      "155000, 2058349, 1, 98.071\n",
      "156000, 10261162, 1, 98.095\n",
      "157000, 10261664, 1, 98.118\n",
      "158000, 10062935, 1, 98.142\n",
      "159000, 10357780, 1, 98.165\n",
      "160000, 1179551, 1, 98.189\n",
      "161000, 10054307, 1, 98.213\n",
      "162000, 625754, 1, 98.236\n",
      "163000, 3739789, 1, 98.260\n",
      "164000, 50411, 1, 98.283\n",
      "165000, 4148327, 1, 98.307\n",
      "166000, 2274515, 1, 98.330\n",
      "167000, 3007129, 1, 98.354\n",
      "168000, 2880818, 1, 98.378\n",
      "169000, 375813, 1, 98.401\n",
      "170000, 10296046, 1, 98.425\n",
      "171000, 705652, 1, 98.448\n",
      "172000, 10017953, 1, 98.472\n",
      "173000, 2281441, 1, 98.496\n",
      "174000, 2138555, 1, 98.519\n",
      "175000, 2647114, 1, 98.543\n",
      "176000, 2336646, 1, 98.566\n",
      "177000, 40524, 1, 98.590\n",
      "178000, 10271023, 1, 98.614\n",
      "179000, 75498, 1, 98.637\n",
      "180000, 10340819, 1, 98.661\n",
      "181000, 912154, 1, 98.684\n",
      "182000, 2315624, 1, 98.708\n",
      "183000, 1211553, 1, 98.731\n",
      "184000, 10387400, 1, 98.755\n",
      "185000, 1180461, 1, 98.779\n",
      "186000, 10171546, 1, 98.802\n",
      "187000, 2190819, 1, 98.826\n",
      "188000, 10139034, 1, 98.849\n",
      "189000, 10122442, 1, 98.873\n",
      "190000, 10209324, 1, 98.897\n",
      "191000, 10285147, 1, 98.920\n",
      "192000, 218391, 1, 98.944\n",
      "193000, 10215611, 1, 98.967\n",
      "194000, 10232815, 1, 98.991\n",
      "195000, 1087970, 1, 99.014\n",
      "196000, 2292376, 1, 99.038\n",
      "197000, 2183982, 1, 99.062\n",
      "198000, 356219, 1, 99.085\n",
      "199000, 4003841, 1, 99.109\n",
      "200000, 10224706, 1, 99.132\n",
      "201000, 2189531, 1, 99.156\n",
      "202000, 195731, 1, 99.180\n",
      "203000, 10013228, 1, 99.203\n",
      "204000, 223303, 1, 99.227\n",
      "205000, 10214779, 1, 99.250\n",
      "206000, 4688413, 1, 99.274\n",
      "207000, 1113052, 1, 99.297\n",
      "208000, 4094708, 1, 99.321\n",
      "209000, 3182542, 1, 99.345\n",
      "210000, 1307780, 1, 99.368\n",
      "211000, 2224503, 1, 99.392\n",
      "212000, 2197279, 1, 99.415\n",
      "213000, 1022157, 1, 99.439\n",
      "214000, 234314, 1, 99.463\n",
      "215000, 10268675, 1, 99.486\n",
      "216000, 10045566, 1, 99.510\n",
      "217000, 2235432, 1, 99.533\n",
      "218000, 10158988, 1, 99.557\n",
      "219000, 2648843, 1, 99.580\n",
      "220000, 116133, 1, 99.604\n",
      "221000, 5042719, 1, 99.628\n",
      "222000, 10114386, 1, 99.651\n",
      "223000, 10125195, 1, 99.675\n",
      "224000, 2299483, 1, 99.698\n",
      "225000, 101746, 1, 99.722\n",
      "226000, 2274919, 1, 99.746\n",
      "227000, 10238306, 1, 99.769\n",
      "228000, 2099986, 1, 99.793\n",
      "229000, 375132, 1, 99.816\n",
      "230000, 2297489, 1, 99.840\n",
      "231000, 10111120, 1, 99.864\n",
      "232000, 2260095, 1, 99.887\n",
      "233000, 10250932, 1, 99.911\n",
      "234000, 2437981, 1, 99.934\n",
      "235000, 741858, 1, 99.958\n",
      "236000, 4234823, 1, 99.981\n"
     ]
    }
   ],
   "source": [
    "total_freq = sum(map(lambda e: len(e), aid_seqs))\n",
    "acc = 0\n",
    "for cnt_idx in range(0, len(aid_counter)):\n",
    "    aid, cnt = aid_counter[cnt_idx]\n",
    "    acc += cnt\n",
    "    if cnt_idx % 1000 == 0:\n",
    "        print(f\"{cnt_idx}, {aid}, {cnt}, {acc * 100 / total_freq:.3f}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.text.Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(aid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[255, 363, 6269, 2536, 1332]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['2302415 2326467 714453 862952 860568 4234823'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena_p3",
   "language": "python",
   "name": "arena_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
