{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import load_json\n",
    "from data_loader.song_info_loader import SongMetaInfoLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "      # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../res/orig/train.json'\n",
    "META_DATA_PATH = '../res/song_meta.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make meta info of songs!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9e0d8f473b4ebabb9df03c287375a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=92056.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0759d0fa364e9b8836fb34ffba4861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=549729.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238c5418f8e04d6c82064f895e9146c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=707989.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train_data = load_json(TRAIN_DATA_PATH)\n",
    "meta_data = SongMetaInfoLoader()\n",
    "meta_data.make_info_map(playlist_data=train_data, meta_data=load_json(META_DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playlist_to_aids(playlists):\n",
    "    aid_list = []\n",
    "    for plays in tqdm(playlists):\n",
    "        aids = list(map(lambda s: str(meta_data.song_album_id_map[s]), plays['songs']))\n",
    "        aid_list.append(aids)\n",
    "        \n",
    "    return aid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7432971a77442e9aab1f00a67496755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=92056.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aid_seqs = playlist_to_aids(playlists=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = [tag for tags in map(lambda e: filter(lambda t: t, e['tags']), train_data) for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = set(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25479"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_in_seq(counts, ratio_thres=0.9, print_freq=1000):\n",
    "    total = sum(map(lambda kc:kc[1], counts))\n",
    "    acc = 0\n",
    "    for cnt_idx in range(0, len(counts)):\n",
    "        val, cnt = counts[cnt_idx]\n",
    "        acc += cnt\n",
    "        ratio = acc / total\n",
    "        if ratio < ratio_thres:\n",
    "            continue\n",
    "        if cnt_idx % print_freq == 0:\n",
    "            print(f\"{cnt_idx}, {val}, {cnt}, {ratio * 100:.3f}\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_counter = Counter([aid for sub_seq in aid_seqs for aid in sub_seq]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000, 2328588, 8, 90.124\n",
      "65000, 10077224, 7, 91.015\n",
      "70000, 4805, 6, 91.806\n",
      "75000, 2296096, 6, 92.514\n",
      "80000, 2657488, 5, 93.124\n",
      "85000, 589457, 5, 93.714\n",
      "90000, 2647078, 4, 94.195\n",
      "95000, 2304888, 4, 94.666\n",
      "100000, 10362390, 3, 95.105\n",
      "105000, 10332514, 3, 95.459\n",
      "110000, 10413570, 3, 95.812\n",
      "115000, 192854, 3, 96.166\n",
      "120000, 10211449, 2, 96.479\n",
      "125000, 2320238, 2, 96.715\n",
      "130000, 10320115, 2, 96.951\n",
      "135000, 352962, 2, 97.187\n",
      "140000, 2278278, 2, 97.422\n",
      "145000, 2291333, 2, 97.658\n",
      "150000, 2175387, 2, 97.894\n",
      "155000, 2058349, 1, 98.071\n",
      "160000, 1179551, 1, 98.189\n",
      "165000, 4148327, 1, 98.307\n",
      "170000, 10296046, 1, 98.425\n",
      "175000, 2647114, 1, 98.543\n",
      "180000, 10340819, 1, 98.661\n",
      "185000, 1180461, 1, 98.779\n",
      "190000, 10209324, 1, 98.897\n",
      "195000, 1087970, 1, 99.014\n",
      "200000, 10224706, 1, 99.132\n",
      "205000, 10214779, 1, 99.250\n",
      "210000, 1307780, 1, 99.368\n",
      "215000, 10268675, 1, 99.486\n",
      "220000, 116133, 1, 99.604\n",
      "225000, 101746, 1, 99.722\n",
      "230000, 2297489, 1, 99.840\n",
      "235000, 741858, 1, 99.958\n"
     ]
    }
   ],
   "source": [
    "freq_in_seq(counts=aid_counter, print_freq=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counter = Counter(train_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900, 퇴근_후, 7, 90.060\n",
      "3000, 기분좋아, 7, 90.244\n",
      "3100, 신나는팝, 7, 90.428\n",
      "3200, 옛날생각, 6, 90.598\n",
      "3300, 연간차트, 6, 90.756\n",
      "3400, 오늘은_뭐듣지, 6, 90.913\n",
      "3500, 정신수양요가음악, 6, 91.071\n",
      "3600, 해질무렵, 5, 91.216\n",
      "3700, 멋짐주의, 5, 91.348\n",
      "3800, 한강에서, 5, 91.479\n",
      "3900, 봄산책, 5, 91.610\n",
      "4000, 남부힙합, 5, 91.742\n",
      "4100, 토이, 5, 91.873\n",
      "4200, 그때, 4, 92.001\n",
      "4300, 들어본적있는노래, 4, 92.106\n",
      "4400, 침대위에서듣는음악, 4, 92.211\n",
      "4500, 첸백시, 4, 92.316\n",
      "4600, 추억에, 4, 92.421\n",
      "4700, 걸그룹모음, 4, 92.526\n",
      "4800, 생각_사색_명상, 4, 92.631\n",
      "4900, 데뷔앨범, 4, 92.736\n",
      "5000, 세계, 4, 92.841\n",
      "5100, 가을겨울, 4, 92.946\n",
      "5200, 타임슬립, 3, 93.025\n",
      "5300, 몸이들썩들썩, 3, 93.104\n",
      "5400, GROOVY한, 3, 93.183\n",
      "5500, 산책하면서, 3, 93.261\n",
      "5600, 생각했던대로, 3, 93.340\n",
      "5700, 마블유니버스, 3, 93.419\n",
      "5800, 돋보이는, 3, 93.498\n",
      "5900, 월드디제이페스티벌, 3, 93.577\n",
      "6000, 객원보컬, 3, 93.655\n",
      "6100, electro, 3, 93.734\n",
      "6200, 훈스디제이, 3, 93.813\n",
      "6300, 라운지_바, 3, 93.892\n",
      "6400, 비오는밤, 3, 93.970\n",
      "6500, 슬픔감성, 3, 94.049\n",
      "6600, 푸디토리움, 3, 94.128\n",
      "6700, 앰비션뮤직, 2, 94.185\n",
      "6800, 건반, 2, 94.237\n",
      "6900, 이것이다, 2, 94.290\n",
      "7000, 흥나는EDM, 2, 94.342\n",
      "7100, 케빈오, 2, 94.395\n",
      "7200, 커버영상, 2, 94.447\n",
      "7300, 락스피릿, 2, 94.500\n",
      "7400, 남자발라더, 2, 94.552\n",
      "7500, 한국래퍼, 2, 94.605\n",
      "7600, 획져, 2, 94.657\n",
      "7700, 클릭비, 2, 94.710\n",
      "7800, 양희은, 2, 94.763\n",
      "7900, 프로듀스_101, 2, 94.815\n",
      "8000, 여성밴드, 2, 94.868\n",
      "8100, 필청트랙, 2, 94.920\n",
      "8200, 스카, 2, 94.973\n",
      "8300, Kehlani, 2, 95.025\n",
      "8400, 투명한, 2, 95.078\n",
      "8500, 힙합라운지, 2, 95.130\n",
      "8600, 라이벌, 2, 95.183\n",
      "8700, 귀성, 2, 95.235\n",
      "8800, 차한잔의여유, 2, 95.288\n",
      "8900, VINXEN, 2, 95.340\n",
      "9000, 졸림, 2, 95.393\n",
      "9100, 마음껏_울어요, 2, 95.445\n",
      "9200, Vangdale, 2, 95.498\n",
      "9300, ChuckBerry, 2, 95.550\n",
      "9400, 한시간, 2, 95.603\n",
      "9500, 스톰프뮤직, 2, 95.655\n",
      "9600, 호불호없는, 2, 95.708\n",
      "9700, 탑텐, 2, 95.760\n",
      "9800, 캐리, 2, 95.813\n",
      "9900, 강한비트, 2, 95.865\n",
      "10000, 해브어나이스데이, 2, 95.918\n",
      "10100, 놓치지마세요, 1, 95.962\n",
      "10200, 떡상, 1, 95.988\n",
      "10300, 봄발라드, 1, 96.014\n",
      "10400, 슈로대, 1, 96.040\n",
      "10500, 등용문, 1, 96.067\n",
      "10600, 8월3주차, 1, 96.093\n",
      "10700, 잠깐, 1, 96.119\n",
      "10800, 어쿠스틱동요, 1, 96.145\n",
      "10900, 케이팝댄스곡, 1, 96.172\n",
      "11000, 이쁜노래, 1, 96.198\n",
      "11100, 재즈힙합추천, 1, 96.224\n",
      "11200, 70년대생, 1, 96.250\n",
      "11300, 일년, 1, 96.277\n",
      "11400, 소울펑크, 1, 96.303\n",
      "11500, 품절남, 1, 96.329\n",
      "11600, 셀프위로, 1, 96.356\n",
      "11700, 최신팝플레이리스트, 1, 96.382\n",
      "11800, 3월_2주차, 1, 96.408\n",
      "11900, 유엔젤보이스, 1, 96.434\n",
      "12000, 충분히_뜰만_한_곡들, 1, 96.461\n",
      "12100, 봄타고, 1, 96.487\n",
      "12200, 트렌치, 1, 96.513\n",
      "12300, 달래주는, 1, 96.539\n",
      "12400, 지역명, 1, 96.566\n",
      "12500, 온다리쿠, 1, 96.592\n",
      "12600, 사랑싸움, 1, 96.618\n",
      "12700, 나를비춰줘, 1, 96.644\n",
      "12800, 광고수록, 1, 96.671\n",
      "12900, 18, 1, 96.697\n",
      "13000, 친구를위해, 1, 96.723\n",
      "13100, 7월2주차, 1, 96.749\n",
      "13200, 소낙눈, 1, 96.776\n",
      "13300, 어반자카파어반, 1, 96.802\n",
      "13400, 사진첩, 1, 96.828\n",
      "13500, 맨하탄, 1, 96.854\n",
      "13600, 신승은, 1, 96.881\n",
      "13700, 진가치들을래, 1, 96.907\n",
      "13800, 어센틱, 1, 96.933\n",
      "13900, 서리, 1, 96.960\n",
      "14000, 치얼업, 1, 96.986\n",
      "14100, 후, 1, 97.012\n",
      "14200, 운동해서, 1, 97.038\n",
      "14300, 쿵치타치, 1, 97.065\n",
      "14400, 리시차, 1, 97.091\n",
      "14500, 겨울에는, 1, 97.117\n",
      "14600, 헤어졌을때, 1, 97.143\n",
      "14700, 여정, 1, 97.170\n",
      "14800, 재니스, 1, 97.196\n",
      "14900, 피가로의결혼, 1, 97.222\n",
      "15000, Doubleyou, 1, 97.248\n",
      "15100, 포크락, 1, 97.275\n",
      "15200, 내적댄스뿜뿜, 1, 97.301\n",
      "15300, 사물놀이, 1, 97.327\n",
      "15400, 끝맺음, 1, 97.353\n",
      "15500, 영기, 1, 97.380\n",
      "15600, 사랑하지만, 1, 97.406\n",
      "15700, 정윤정, 1, 97.432\n",
      "15800, 둠바둠바, 1, 97.458\n",
      "15900, 더우시죠, 1, 97.485\n",
      "16000, 펫뮤직, 1, 97.511\n",
      "16100, 사랑하고싶은사람, 1, 97.537\n",
      "16200, 인도음악, 1, 97.564\n",
      "16300, 손, 1, 97.590\n",
      "16400, 201709, 1, 97.616\n",
      "16500, 가을과당신, 1, 97.642\n",
      "16600, KingOfSoul, 1, 97.669\n",
      "16700, Chill_2, 1, 97.695\n",
      "16800, yoyo, 1, 97.721\n",
      "16900, 클래지콰이, 1, 97.747\n",
      "17000, 유명한작곡가, 1, 97.774\n",
      "17100, 허슬, 1, 97.800\n",
      "17200, 글로벌, 1, 97.826\n",
      "17300, 하얀색, 1, 97.852\n",
      "17400, 자코파스토리우스, 1, 97.879\n",
      "17500, 이사오사사키, 1, 97.905\n",
      "17600, 야나두, 1, 97.931\n",
      "17700, 저녁에들으면좋은클래식, 1, 97.957\n",
      "17800, 아슬아슬, 1, 97.984\n",
      "17900, 자동차광고, 1, 98.010\n",
      "18000, 출출해, 1, 98.036\n",
      "18100, 시원한비트, 1, 98.062\n",
      "18200, Bach, 1, 98.089\n",
      "18300, t설렘, 1, 98.115\n",
      "18400, 마음이텅빈날, 1, 98.141\n",
      "18500, Bon, 1, 98.168\n",
      "18600, 먼데이, 1, 98.194\n",
      "18700, 분수, 1, 98.220\n",
      "18800, 썬데이상담쏭, 1, 98.246\n",
      "18900, 레이브릭스, 1, 98.273\n",
      "19000, 돈나오는노래, 1, 98.299\n",
      "19100, LupeFiasco, 1, 98.325\n",
      "19200, 센슈얼, 1, 98.351\n",
      "19300, 지침서, 1, 98.378\n",
      "19400, 크리스마스겨울추천클래식, 1, 98.404\n",
      "19500, 듣기편한음악, 1, 98.430\n",
      "19600, 마이틴, 1, 98.456\n",
      "19700, 따스한_노래와함께, 1, 98.483\n",
      "19800, 지구멸망, 1, 98.509\n",
      "19900, 행복하더라도, 1, 98.535\n",
      "20000, ㄹ, 1, 98.561\n",
      "20100, 댄스신동, 1, 98.588\n",
      "20200, 어쩌면우울, 1, 98.614\n",
      "20300, 트렌디이디엠, 1, 98.640\n",
      "20400, 한스밴드, 1, 98.666\n",
      "20500, mingbb, 1, 98.693\n",
      "20600, 날, 1, 98.719\n",
      "20700, 아무생각없다, 1, 98.745\n",
      "20800, 분위기있는R, 1, 98.772\n",
      "20900, 찰리파커, 1, 98.798\n",
      "21000, 나들이송, 1, 98.824\n",
      "21100, EdSheeraned, 1, 98.850\n",
      "21200, 녹두전, 1, 98.877\n",
      "21300, 김상관, 1, 98.903\n",
      "21400, 근손실오기전에, 1, 98.929\n",
      "21500, ahn, 1, 98.955\n",
      "21600, 사람목소리맞나, 1, 98.982\n",
      "21700, 추천발라드모음, 1, 99.008\n",
      "21800, Skarbo음반사클래식, 1, 99.034\n",
      "21900, 프롬공연, 1, 99.060\n",
      "22000, 올드명곡, 1, 99.087\n",
      "22100, 낭만적인식사, 1, 99.113\n",
      "22200, 더운날씨에딱, 1, 99.139\n",
      "22300, 애프터클럽, 1, 99.165\n",
      "22400, 망플, 1, 99.192\n",
      "22500, 커피숍매장호텔로비추천클래식, 1, 99.218\n",
      "22600, 둠바둠, 1, 99.244\n",
      "22700, LucyRose, 1, 99.270\n",
      "22800, 미국밴드, 1, 99.297\n",
      "22900, 와블, 1, 99.323\n",
      "23000, 그리고5분후, 1, 99.349\n",
      "23100, 록밴드출신, 1, 99.376\n",
      "23200, 128비트, 1, 99.402\n",
      "23300, 왕자의게임, 1, 99.428\n",
      "23400, 남자뮤지션, 1, 99.454\n",
      "23500, 임형주, 1, 99.481\n",
      "23600, standbyyourmen, 1, 99.507\n",
      "23700, 잠이안오는밤, 1, 99.533\n",
      "23800, 팝콘, 1, 99.559\n",
      "23900, Ariana, 1, 99.586\n",
      "24000, 주말에커피한잔, 1, 99.612\n",
      "24100, 90년대디바, 1, 99.638\n",
      "24200, CeeLo, 1, 99.664\n",
      "24300, 개가수, 1, 99.691\n",
      "24400, 지음, 1, 99.717\n",
      "24500, 미카, 1, 99.743\n",
      "24600, 슈퍼그룹, 1, 99.769\n",
      "24700, 기타의신, 1, 99.796\n",
      "24800, 본딩, 1, 99.822\n",
      "24900, 가요무대, 1, 99.848\n",
      "25000, 인듯, 1, 99.874\n",
      "25100, 찾고싶었다, 1, 99.901\n",
      "25200, 누군가떠오를때, 1, 99.927\n",
      "25300, 중후함, 1, 99.953\n",
      "25400, 적폐청산, 1, 99.980\n"
     ]
    }
   ],
   "source": [
    "freq_in_seq(counts=tag_counter, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.preprocessing.text.Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_cnt_thres = 60000\n",
    "aid_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=aid_cnt_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_tokenizer.fit_on_texts(aid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[255, 363, 6269, 2536, 1332]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aid_tokenizer.texts_to_sequences(['2302415 2326467 714453 862952 860568 4234823'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aid_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in tag_counter:\n",
    "    if e[0] == '':\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_cnt_thres = 3000\n",
    "candidate_tags = set(map(lambda tc:tc[0], tag_counter[:tag_cnt_thres]))\n",
    "filtered_train = []\n",
    "for train in train_data:\n",
    "    valid_tags = filter(lambda e: e in candidate_tags, train['tags'])\n",
    "    train['tags'] = list(valid_tags)\n",
    "    if len(train['tags']) > 0:\n",
    "        filtered_train.append(train)\n",
    "# filtered_train = list(filter(lambda t: len(list(filter(lambda e: e in candidate_tags, t['tags']))) > 0, train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tags': ['힐링', '휴식', '밤', '새벽'],\n",
       " 'id': 147668,\n",
       " 'plylst_title': 'To. 힘들고 지친 분들에게',\n",
       " 'songs': [663185,\n",
       "  649626,\n",
       "  6855,\n",
       "  188486,\n",
       "  348451,\n",
       "  169945,\n",
       "  512599,\n",
       "  532114,\n",
       "  454528,\n",
       "  418935,\n",
       "  124485,\n",
       "  517372,\n",
       "  549950,\n",
       "  540588,\n",
       "  500931,\n",
       "  233641,\n",
       "  331055,\n",
       "  490266,\n",
       "  268515,\n",
       "  531820,\n",
       "  413762,\n",
       "  422713,\n",
       "  215080,\n",
       "  413189,\n",
       "  577903,\n",
       "  352228,\n",
       "  630395,\n",
       "  539109,\n",
       "  152475,\n",
       "  111865,\n",
       "  7460,\n",
       "  72432,\n",
       "  572480,\n",
       "  348092,\n",
       "  324208,\n",
       "  186039,\n",
       "  376140,\n",
       "  270269,\n",
       "  622615,\n",
       "  35001,\n",
       "  444706,\n",
       "  491303,\n",
       "  408698,\n",
       "  325979,\n",
       "  25538,\n",
       "  549392,\n",
       "  473514,\n",
       "  666814,\n",
       "  118223,\n",
       "  697100,\n",
       "  333034,\n",
       "  359279,\n",
       "  421124,\n",
       "  403253,\n",
       "  27784,\n",
       "  118049,\n",
       "  339124,\n",
       "  175073,\n",
       "  522895,\n",
       "  6925,\n",
       "  615815,\n",
       "  672550,\n",
       "  379112,\n",
       "  80972,\n",
       "  227036,\n",
       "  112153],\n",
       " 'like_cnt': 12,\n",
       " 'updt_date': '2016-06-23 10:06:27.000'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_encoder = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_encoder.fit(map(lambda e: e['tags'], filtered_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00년대', '0살', '1', ..., '힙합음악', '힙합클럽', '힙합트랙'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_y(dict_list, max_seq):\n",
    "    aids_list = playlist_to_aids(dict_list)\n",
    "    tag_list = list(map(lambda e: e['tags'], dict_list))\n",
    "    x_list = tf.keras.preprocessing.sequence.pad_sequences(sequences=aid_tokenizer.texts_to_sequences(aids_list), \n",
    "                                                           maxlen=max_seq, padding='pre')\n",
    "    y_list = tag_encoder.transform(tag_list)\n",
    "    \n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f43d9d2315e44a9a199bf8efbcc5fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90143.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = make_x_y(dict_list=filtered_train, max_seq=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  122   120  8601  1088  1758   266  2088  1259  1306  3921    89  1439\n",
      "   249   280   109  1850   420  5552  5042  1292 10161   605   255   980\n",
      "  2075 17076  3068   272   363  7469  3688   258]\n",
      "(90143, 32)\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343636\n",
      "4\n",
      "(90143, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y))\n",
    "print(np.sum(y[0]))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "class AlbumEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81128, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq = 32\n",
    "embed_dim = 8  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 4  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_seq, aid_cnt_thres, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(len(tag_encoder.classes_), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 32, 8)             480256    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 32, 8)             396       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3000)              195000    \n",
      "=================================================================\n",
      "Total params: 676,228\n",
      "Trainable params: 676,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "model.compile(metrics=[\"accuracy\"], optimizer=optim, loss=keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81128 samples, validate on 9015 samples\n",
      "Epoch 1/10\n",
      "81128/81128 [==============================] - 19s 235us/sample - loss: 22.2827 - accuracy: 0.0899 - val_loss: 21.6100 - val_accuracy: 0.1100\n",
      "Epoch 2/10\n",
      "81128/81128 [==============================] - 16s 194us/sample - loss: 20.8959 - accuracy: 0.1177 - val_loss: 20.9696 - val_accuracy: 0.1380\n",
      "Epoch 3/10\n",
      "81128/81128 [==============================] - 16s 195us/sample - loss: 20.3441 - accuracy: 0.1439 - val_loss: 20.7425 - val_accuracy: 0.1449\n",
      "Epoch 4/10\n",
      "81128/81128 [==============================] - 16s 196us/sample - loss: 20.0307 - accuracy: 0.1549 - val_loss: 20.6393 - val_accuracy: 0.1530\n",
      "Epoch 5/10\n",
      "81128/81128 [==============================] - 16s 196us/sample - loss: 19.8003 - accuracy: 0.1677 - val_loss: 20.5669 - val_accuracy: 0.1598\n",
      "Epoch 6/10\n",
      "81128/81128 [==============================] - 16s 195us/sample - loss: 19.6400 - accuracy: 0.1780 - val_loss: 20.3787 - val_accuracy: 0.1752\n",
      "Epoch 7/10\n",
      "81128/81128 [==============================] - 16s 192us/sample - loss: 19.5247 - accuracy: 0.1832 - val_loss: 20.3690 - val_accuracy: 0.1712\n",
      "Epoch 8/10\n",
      "81128/81128 [==============================] - 16s 195us/sample - loss: 19.4467 - accuracy: 0.1889 - val_loss: 20.3506 - val_accuracy: 0.1799\n",
      "Epoch 9/10\n",
      "81128/81128 [==============================] - 16s 192us/sample - loss: 19.3843 - accuracy: 0.1924 - val_loss: 20.3718 - val_accuracy: 0.1827\n",
      "Epoch 10/10\n",
      "81128/81128 [==============================] - 16s 199us/sample - loss: 19.3336 - accuracy: 0.1946 - val_loss: 20.3531 - val_accuracy: 0.1829\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=64, epochs=10, validation_data=(x_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred import Experiment\n",
    "from sacred.observers import MongoObserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = Experiment('kakao-arena', interactive=True)\n",
    "mongo_url = '172.28.10.4:27017'  # Or <server-static-ip>:<port> if running on server\n",
    "ex.observers.append(MongoObserver.create(url=mongo_url,\n",
    "                                         db_name='omniboard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ex.config\n",
    "def model_config():\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.1-p3.7",
   "language": "python",
   "name": "tf-2.1-p3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
